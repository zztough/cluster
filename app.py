import streamlit as st
import spacy
from spacy_streamlit import visualize_ner
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, Birch
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, davies_bouldin_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib # For font management
import matplotlib.font_manager # For font management
import os # For checking font file existence
from scipy.cluster.hierarchy import dendrogram, linkage

# A simple text classifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

# New imports for advanced visualizations
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go

# --- Page Configuration ---
st.set_page_config(page_title="ä¸­æ–‡NLPæ™ºèƒ½åˆ†æå¹³å°", page_icon="ğŸ¤–", layout="wide")

# --- Custom CSS for Dark Theme (OpenAI-like) ---
custom_css = """
<style>
    /* Base styles */
    body {
        font-family: 'Noto Sans CJK SC', 'Helvetica Neue', Arial, sans-serif; /* Ensure CJK font is prioritized */
        color: #D1D5DB; /* Light gray text */
        background-color: #0F172A; /* Dark blue-gray background */
    }
    .stApp {
        background-color: #0F172A; /* Dark blue-gray background */
    }

    /* Titles and Headers - Unified Sizes */
    h1, h2, h3, h4, h5 {
        color: #F3F4F6; /* Lighter text for headers */
    }
    h1 { /* For the main page title in render_homepage */
        text-align: center;
        padding-bottom: 20px;
        font-size: 2.6em; /* Unified */
        font-weight: 600;
        border-bottom: 2px solid #374151;
    }
    h2 { /* Main section headers (st.header for "1. è¾“å…¥æ–‡æœ¬", "2. æŸ¥çœ‹åˆ†æç»“æœ") */
        margin-top: 2rem;
        margin-bottom: 1rem;
        padding-bottom: 0.5rem;
        font-size: 2.0em; /* Unified & Increased */
        font-weight: 500;
        border-bottom: 1px solid #374151;
    }
    h3 { /* Sub-section headers (st.subheader for "âœ‚ï¸ ä¸­æ–‡åˆ†è¯ç»“æœ", "ğŸ”¦ é«˜äº®å®ä½“:") */
        font-size: 1.7em; /* Unified & Increased */
        font-weight: 500;
        color: #E5E7EB;
        margin-top: 1.5rem;
        margin-bottom: 0.8rem;
    }
    h4 { /* Sub-sub-sections (e.g., "ğŸ“‹ è¯†åˆ«åˆ°çš„å®ä½“åˆ—è¡¨", "ğŸ“ èšç±»è¯„ä¼°æŒ‡æ ‡") */
        font-size: 1.4em; /* Unified & Increased */
        font-weight: 500;
        color: #E0E0E0;
        margin-top: 1.5rem;
        margin-bottom: 0.7rem;
        padding-bottom: 0.4rem;
        border-bottom: 1px dashed #4B5563;
    }
    h5 { /* Smaller titles (e.g., word frequency, KMeans/DBSCAN å‚æ•°) */
        font-size: 1.2em; /* Unified & Increased */
        font-weight: 500;
        color: #E5E7EB;
        margin-top: 1rem;
        margin-bottom: 0.5rem;
    }

    /* Sidebar styling */
    div[data-testid="stSidebar"] > div:first-child {
        background-color: #1E293B;
        border-right: 1px solid #374151;
    }
    div[data-testid="stSidebar"] .stRadio > label { /* Sidebar Navigation Radio Label */
        font-size: 1.2em !important; /* Increased */
        font-weight: 500;
        color: #E5E7EB;
        padding-bottom: 5px; /* Add some space below the main label */
    }
     div[data-testid="stSidebar"] .stRadio div[role="radiogroup"] > label { /* Sidebar Radio button options */
        padding: 7px 0px !important; /* Increased padding */
        font-size: 1.1em !important; /* Increased */
    }
    div[data-testid="stSidebar"] p { /* Sidebar text like Â© 2024 */
        color: #9CA3AF;
    }
     div[data-testid="stSidebar"] h1, /* Sidebar Header */
     div[data-testid="stSidebar"] h2,
     div[data-testid="stSidebar"] h3,
     div[data-testid="stSidebar"] h4 {
        color: #F3F4F6;
     }

    /* Input widgets styling - Increase label font size for main content area */
    .main div[data-testid="stTextInput"] label,
    .main div[data-testid="stTextArea"] label,
    .main div[data-testid="stSelectbox"] label,
    .main div[data-testid="stNumberInput"] label,
    .main div[data-testid="stRadio"] > label, /* For main content radio buttons */
    .main div[data-testid="stFileUploader"] label,
    .main div[data-testid="stDateInput"] label,
    .main div[data-testid="stTimeInput"] label,
    .main div[data-testid="stColorPicker"] label,
    .main div[data-testid="stSlider"] label {
        font-size: 1.15em !important; /* Increased font size for widget labels in main area */
        color: #E5E7EB !important;
        margin-bottom: 0.4rem !important; 
    }

    div[data-testid="stTextInput"] input,
    div[data-testid="stTextArea"] textarea,
    div[data-testid="stSelectbox"] div[data-baseweb="select"] > div, /* Selectbox */
    div[data-testid="stNumberInput"] input {
        background-color: #374151 !important; /* Darker input background */
        color: #F3F4F6 !important; /* Light text in inputs */
        border: 1px solid #4B5563 !important;
        border-radius: 0.375rem !important; /* Tailwind-like rounded-md */
    }
    div[data-testid="stTextArea"] textarea {
        min-height: 150px; /* Ensure text area is reasonably sized */
    }

    /* Button styling */
    div[data-testid="stButton"] > button {
        background-color: #2563EB; /* OpenAI-like blue */
        color: white;
        border: none;
        padding: 0.6rem 1.2rem;
        border-radius: 0.375rem;
        font-weight: 500;
        transition: background-color 0.2s ease;
    }
    div[data-testid="stButton"] > button:hover {
        background-color: #1D4ED8; /* Darker blue on hover */
    }
    div[data-testid="stButton"] > button:focus {
        outline: none !important;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.5) !important; /* Blue focus ring */
    }
    
    /* Radio buttons in main content */
    div[data-testid="stRadio"] label {
        font-size: 1em;
    }

    /* Slider styling */
    div[data-testid="stSlider"] {
        /* color: #60A5FA; */ /* Blue for slider track/thumb - Use Streamlit's default theming for this if possible */
    }


    /* Expander styling */
    .st-expander {
        border: 1px solid #374151;
        border-radius: 0.5rem; /* Tailwind rounded-lg */
        /* background-color: #1E293B; */ /* Background for expander content area - Let Streamlit handle content bg */
    }
    .st-expander header {
        background-color: #374151; /* Header of expander */
        color: #F3F4F6;
        padding: 0.75rem 1rem;
        border-top-left-radius: 0.5rem;
        border-top-right-radius: 0.5rem;
        border-bottom: 1px solid #1E293B; /* Separator for header */
        font-size: 1.25em !important; /* Increased font size for expander headers */
        font-weight: 500 !important;
    }
    .st-expander header:hover {
        background-color: #4B5563;
    }
    .st-expander div[data-testid="stExpanderDetails"] {
         background-color: #1E293B; /* Content area of expander */
         padding: 1rem;
         border-bottom-left-radius: 0.5rem;
         border-bottom-right-radius: 0.5rem;
    }


    /* Dataframes, Tables, Info/Warning/Error boxes */
    div[data-testid="stDataFrame"],
    div[data-testid="stTable"] { /* st.table is not used, but good to have */
        border-radius: 0.375rem;
        overflow: hidden; /* Ensures border-radius clips content */
    }
    div[data-testid="stInfo"],
    div[data-testid="stWarning"],
    div[data-testid="stError"],
    div[data-testid="stSuccess"] {
        border-radius: 0.375rem;
        padding: 1rem;
        color: #F3F4F6; /* Ensure text is light */
    }
    div[data-testid="stInfo"] { background-color: rgba(59, 130, 246, 0.2); border-left: 5px solid #3B82F6; } /* Blueish Info */
    div[data-testid="stSuccess"] { background-color: rgba(16, 185, 129, 0.2); border-left: 5px solid #10B981; } /* Greenish Success */
    div[data-testid="stWarning"] { background-color: rgba(245, 158, 11, 0.2); border-left: 5px solid #F59E0B; } /* Amber/Orange Warning */
    div[data-testid="stError"] { background-color: rgba(239, 68, 68, 0.2); border-left: 5px solid #EF4444; } /* Reddish Error */


    /* Metric styling */
    div[data-testid="stMetric"] {
        background-color: #1E293B;
        border: 1px solid #374151;
        padding: 1rem 1.5rem;
        border-radius: 0.5rem;
    }
    div[data-testid="stMetric"] > label { /* Metric label */
        color: #9CA3AF; /* Muted label color */
        font-size: 0.9em;
    }
    div[data-testid="stMetric"] > div[data-testid="stMetricValue"] { /* Metric value */
        color: #F3F4F6;
        font-size: 2em; 
        font-weight: 600;
    }
    
    /* Plotly chart container - make sure it blends */
    div[data-testid="stPlotlyChart"] {
        border-radius: 0.375rem;
        overflow: hidden; /* Clip contents like corners */
    }

    /* Matplotlib chart container */
    div[data-testid="stImage"] > img { /* For st.pyplot */
         border-radius: 0.375rem;
         background-color: transparent; /* Ensure image background doesn't override */
    }
    
    /* Horizontal Divider */
    hr {
        border-top: 1px solid #374151;
    }

    /* Links */
    a {
        color: #60A5FA; /* Light blue for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
        color: #93C5FD; /* Lighter blue on hover */
    }
    
    /* Feature card styling for homepage */
    .feature-card {
        background-color: #1E293B; 
        padding: 1.5rem;
        border-radius: 0.5rem; 
        border: 1px solid #374151;
        margin-bottom: 1.2rem; /* Increased margin */
        transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        height: calc(100% - 1.2rem); /* Fill height minus margin for better alignment */
        display: flex; /* For vertical alignment of content */
        flex-direction: column; /* Stack content vertically */
    }
    .feature-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 8px 16px rgba(0,0,0,0.25); /* Enhanced shadow */
    }
    .feature-card h3 { 
        font-size: 1.4em; /* Slightly larger */
        color: #F3F4F6;
        margin-top: 0;
        margin-bottom: 0.75rem; /* Increased margin */
        border-bottom: none; 
        line-height: 1.3;
    }
    .feature-card p { 
        font-size: 0.95em;
        color: #D1D5DB;
        line-height: 1.6;
        flex-grow: 1; /* Allow p to take available space */
    }
    /* End of Feature card styling */

    /* Ensure Streamlit's default spinner is visible on dark background */
    .stSpinner > div {
        border-top-color: #2563EB !important; /* Primary button color for spinner */
        border-right-color: transparent !important;
        border-bottom-color: transparent !important;
        border-left-color: transparent !important;
    }

</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

# --- Model Loading ---
@st.cache_resource # Use st.cache_resource for models
def load_spacy_model(model_name="zh_core_web_sm"):
    """åŠ è½½spaCyæ¨¡å‹ï¼Œå¦‚æœæœªæ‰¾åˆ°æˆ–å‘ç”Ÿå…¶ä»–é”™è¯¯åˆ™æç¤ºå¹¶åœæ­¢ã€‚"""
    try:
        nlp = spacy.load(model_name)
        return nlp
    except OSError:
        st.error(f"SpaCyæ¨¡å‹ '{model_name}' æœªæ‰¾åˆ°ã€‚è¯·åœ¨ç»ˆç«¯è¿è¡Œ: python -m spacy download {model_name}")
        st.stop()
    except Exception as e:
        st.error(f"åŠ è½½SpaCyæ¨¡å‹ '{model_name}' æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        st.error("è¿™å¯èƒ½æ˜¯ç”±äºä¾èµ–åº“ç‰ˆæœ¬ä¸å…¼å®¹ï¼ˆä¾‹å¦‚ NumPy ä¸ spaCy/thinc çš„å…¼å®¹æ€§é—®é¢˜ï¼‰æˆ–å…¶ä»–é…ç½®é—®é¢˜ã€‚è¯·æ£€æŸ¥æ§åˆ¶å°è¾“å‡ºä»¥è·å–è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶ç¡®ä¿æ‚¨çš„ç¯å¢ƒé…ç½®æ­£ç¡®ã€‚")
        st.info("å°è¯•è§£å†³æ–¹æ¡ˆï¼š1. æ£€æŸ¥NumPyç‰ˆæœ¬ (å»ºè®® < 2.0)ã€‚ 2. é‡æ–°å®‰è£…ç›¸å…³åº“ã€‚")
        st.stop()
    # st.stop() should prevent execution from reaching here if an exception occurred.
    # Adding a fallback return or raise for extreme defensiveness, though st.stop() should suffice.
    # This line should ideally not be reached if st.stop() works as intended.
    st.error("SpaCyæ¨¡å‹åŠ è½½å¤±è´¥åï¼Œè„šæœ¬æ„å¤–ç»§ç»­æ‰§è¡Œã€‚è¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—ã€‚")
    return None # Should not be reached if st.stop() is effective

# nlp_spacy = load_spacy_model() # This global instance might need to be re-evaluated or removed if model is chosen dynamically per task
# For NER, we will call load_spacy_model with the selected model name.

# --- Sample Data and Classifier Training (Minimalistic) ---
# æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸åŸºç¡€çš„åˆ†ç±»å™¨ï¼Œä»…ç”¨äºæ¼”ç¤ºã€‚å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤§ã€æ›´å‡è¡¡çš„æ•°æ®é›†å’Œæ›´å¤æ‚çš„æ¨¡å‹ã€‚
classification_texts = [
    "å›½è¶³åœ¨ä¸–ç•Œæ¯é¢„é€‰èµ›ä¸­å–å¾—å…³é”®èƒœåˆ©ï¼Œä½“è‚²è¿·æ¬¢æ¬£é¼“èˆã€‚",
    "æœ€æ–°çš„å…¨çƒç»æµæŠ¥å‘ŠæŒ‡å‡ºï¼Œè‚¡å¸‚é¢ä¸´å›è°ƒé£é™©ï¼Œè´¢ç»é¢†åŸŸéœ€è°¨æ…ã€‚",
    "æŸæ˜æ˜Ÿæ–°ç”µå½±ç¥¨æˆ¿å¤§å–ï¼Œå¼•å‘å¨±ä¹ç•Œçƒ­è®®ã€‚",
    "äººå·¥æ™ºèƒ½æŠ€æœ¯å–å¾—æ–°çªç ´ï¼Œç§‘æŠ€å…¬å¸çº·çº·å¸ƒå±€ã€‚",
    "å¥¥è¿ä¼šä¸­å›½ä»£è¡¨å›¢å†æ·»é‡‘ç‰Œï¼Œä½“è‚²å¥å„¿è¡¨ç°å‡ºè‰²ã€‚",
    "å¤®è¡Œå®£å¸ƒé™æ¯ï¼Œæ—¨åœ¨åˆºæ¿€ç»æµå¢é•¿ï¼Œè´¢ç»å¸‚åœºååº”ç§¯æã€‚",
    "å¹´åº¦éŸ³ä¹ç››å…¸è½ä¸‹å¸·å¹•ï¼Œå¤šä½æ­Œæ‰‹è·å¥–ï¼Œå¨±ä¹æ°›å›´æµ“åšã€‚",
    "æ–°å‹èŠ¯ç‰‡å‘å¸ƒï¼Œè®¡ç®—èƒ½åŠ›å¤§å¹…æå‡ï¼Œç§‘æŠ€åˆ›æ–°æ°¸æ— æ­¢å¢ƒã€‚"
]
classification_labels = ["ä½“è‚²", "è´¢ç»", "å¨±ä¹", "ç§‘æŠ€", "ä½“è‚²", "è´¢ç»", "å¨±ä¹", "ç§‘æŠ€"]

@st.cache_resource # Cache the trained classifier
def train_text_classifier(texts, labels, classifier_choice="MultinomialNB"):
    """è®­ç»ƒä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†ç±»å™¨ï¼Œå¯é€‰æ‹©åˆ†ç±»ç®—æ³•ã€‚"""
    
    selected_classifier = None
    if classifier_choice == "MultinomialNB":
        selected_classifier = MultinomialNB()
    elif classifier_choice == "LogisticRegression":
        selected_classifier = LogisticRegression(random_state=42, solver='liblinear')
    elif classifier_choice == "LinearSVC":
        selected_classifier = LinearSVC(random_state=42, dual='auto')
    else:
        st.error(f"æœªçŸ¥çš„åˆ†ç±»å™¨é€‰é¡¹: {classifier_choice}ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„MultinomialNBã€‚")
        selected_classifier = MultinomialNB()

    # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯çš„TF-IDFå‘é‡åŒ–å™¨
    model = Pipeline([
        ('tfidf', TfidfVectorizer(tokenizer=lambda x: list(jieba.cut(x)))), 
        ('clf', selected_classifier)
    ])
    model.fit(texts, labels)
    return model

# classifier = train_text_classifier(classification_texts, classification_labels) # Deferred to be dynamic based on selection
class_names = sorted(list(set(classification_labels))) # è·å–å”¯ä¸€çš„ç±»åˆ«åå¹¶æ’åº

# --- Matplotlib Font Fix for Chinese ---
# For best results, place a Chinese font file (e.g., SimHei.ttf or NotoSansCJKsc-Regular.otf)
# in the 'nlpp/' directory alongside app.py.
CHINESE_FONT_FILENAME = "NotoSansCJKsc-Regular.otf" # Or your chosen font file

def setup_matplotlib_font():
    try:
        font_path = os.path.join(os.path.dirname(__file__), CHINESE_FONT_FILENAME)
        
        font_successfully_set = False
        if os.path.exists(font_path):
            # Use the font file directly if it exists
            matplotlib.font_manager.fontManager.addfont(font_path)
            font_prop = matplotlib.font_manager.FontProperties(fname=font_path)
            matplotlib.rcParams['font.family'] = font_prop.get_name()
            # Prepend to sans-serif list as well
            matplotlib.rcParams['font.sans-serif'] = [font_prop.get_name()] + matplotlib.rcParams.get('font.sans-serif', [])
            # st.sidebar.info(f"å·²æˆåŠŸåŠ è½½å­—ä½“: {font_prop.get_name()} (æ¥è‡ªæ–‡ä»¶: {CHINESE_FONT_FILENAME})")
            font_successfully_set = True
        else:
            # If local font file not found, try a list of common Chinese font family names
            # These names must be known to the system's Matplotlib font manager
            common_chinese_fonts = [
                'WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'SimHei', 
                'Source Han Sans SC', 'AR PL UKai CN', 'AR PL UMing CN', 
                'WenQuanYi Zen Hei', 'DejaVu Sans Fallback' 
            ]
            
            original_sans_serif = list(matplotlib.rcParams.get('font.sans-serif', [])) # Make a mutable copy
            
            # Check if any of the common Chinese fonts are available and set it as the primary family
            font_manager = matplotlib.font_manager.fontManager
            available_font_names = [f.name for f in font_manager.ttflist]
            
            for font_name in common_chinese_fonts:
                if font_name in available_font_names:
                    matplotlib.rcParams['font.family'] = font_name
                    # Ensure this font is also at the start of the sans-serif list
                    if font_name in original_sans_serif:
                        original_sans_serif.remove(font_name)
                    matplotlib.rcParams['font.sans-serif'] = [font_name] + original_sans_serif
                    # st.sidebar.info(f"å·²æˆåŠŸåŠ è½½ç³»ç»Ÿå­—ä½“: {font_name}")
                    font_successfully_set = True
                    break
            
            if not font_successfully_set:
                st.sidebar.warning(
                    f"æœªåœ¨ '{font_path}' æ‰¾åˆ°å­—ä½“æ–‡ä»¶ '{CHINESE_FONT_FILENAME}'ï¼Œä¸”æœªèƒ½ä»å¸¸è§ç³»ç»Ÿå­—ä½“åˆ—è¡¨ä¸­åŠ è½½ä¸­æ–‡å­—ä½“ã€‚"
                    f"å›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºã€‚è¯·æ”¾ç½®ä¸€ä¸ªTTFä¸­æ–‡å­—ä½“ (å¦‚ SimHei.ttf) åˆ° 'nlpp/' ç›®å½•ä¸‹å¹¶é‡å¯åº”ç”¨ã€‚"
                )

        matplotlib.rcParams['axes.unicode_minus'] = False  # Ensure minus sign displays correctly

        # Dark theme adjustments for Matplotlib
        matplotlib.rcParams['text.color'] = '#E5E7EB'       # Light gray for text
        matplotlib.rcParams['axes.labelcolor'] = '#D1D5DB'  # Slightly darker for labels
        matplotlib.rcParams['xtick.color'] = '#9CA3AF'      # Muted for ticks
        matplotlib.rcParams['ytick.color'] = '#9CA3AF'
        matplotlib.rcParams['axes.edgecolor'] = '#4B5563'   # Border color for axes
        matplotlib.rcParams['figure.facecolor'] = 'none'    # Transparent figure background
        matplotlib.rcParams['axes.facecolor'] = 'none'      # Transparent axes background
        matplotlib.rcParams['savefig.transparent'] = True   # Ensure saved figures are transparent

    except Exception as e:
        st.sidebar.error(f"é…ç½®Matplotlibä¸­æ–‡å­—ä½“æ—¶å‡ºé”™: {e}")

setup_matplotlib_font() # Call the function to set the font

# --- Plotting Functions (Refactored and Enhanced) ---

def plot_scatter_clusters(ax, X_reduced, labels, cluster_model_name,
                          colormap='viridis', marker_size=50, marker_alpha=0.7,
                          marker_styles_enabled=False,
                          bg_color='rgba(0,0,0,0)', show_grid=True, grid_color='#4B5563'):
    """ç»˜åˆ¶èšç±»ç»“æœçš„æ•£ç‚¹å›¾ï¼Œå…·æœ‰å¢å¼ºçš„å¯è§†åŒ–é€‰é¡¹ã€‚"""
    ax.clear() # Clear previous plot on the axis
    ax.set_facecolor(bg_color)

    unique_labels = sorted(list(set(labels)))
    is_dbscan_with_outliers = cluster_model_name == "DBSCAN" and -1 in unique_labels
    
    current_cmap = plt.get_cmap(colormap)
    
    plot_labels_for_colors = [l for l in unique_labels if l != -1] if is_dbscan_with_outliers else unique_labels
    if not plot_labels_for_colors: # Handle cases with only noise or no clusters
        plot_labels_for_colors = unique_labels # Avoid error if only noise points
        
    num_plot_clusters = max(1, len(plot_labels_for_colors)) # Ensure at least 1 for color mapping
    colors_for_plot = current_cmap(np.linspace(0, 1, num_plot_clusters))
    color_map_dict = {label: colors_for_plot[i] for i, label in enumerate(plot_labels_for_colors)}

    marker_options = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'H', 'X']
    legend_elements = []

    for i, label_val in enumerate(unique_labels):
        idx = (labels == label_val)
        current_marker = marker_options[i % len(marker_options)] if marker_styles_enabled else 'o'
        
        if label_val == -1 and is_dbscan_with_outliers:
            ax.scatter(X_reduced[idx, 0], X_reduced[idx, 1], color='gray', label='ç¦»ç¾¤ç‚¹', 
                       alpha=marker_alpha, marker='x', s=marker_size)
            legend_elements.append(matplotlib.lines.Line2D([0], [0], marker='x', color='w', label='ç¦»ç¾¤ç‚¹', markerfacecolor='gray', markersize=10, linestyle='None'))
        elif X_reduced[idx].shape[0] > 0 : # Ensure there are points in the cluster
            cluster_color = color_map_dict.get(label_val, current_cmap(0.0)) # Default color if label somehow not in dict
            ax.scatter(X_reduced[idx, 0], X_reduced[idx, 1], color=cluster_color, 
                       label=f'èšç±» {label_val}', alpha=marker_alpha, 
                       marker=current_marker, edgecolors='w', s=marker_size)
            legend_elements.append(matplotlib.lines.Line2D([0], [0], marker=current_marker, color='w', label=f'èšç±» {label_val}', markerfacecolor=cluster_color, markersize=10))

    if legend_elements:
        ax.legend(handles=legend_elements, title="å›¾ä¾‹", bbox_to_anchor=(1.05, 1), loc='upper left')
    
    ax.set_xlabel("Component 1", fontsize=10)
    ax.set_ylabel("Component 2", fontsize=10)
    
    if show_grid:
        ax.grid(True, color=grid_color, linestyle='--', alpha=0.5)
    else:
        ax.grid(False)
    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend

def plot_dynamic_dendrogram(ax, tfidf_matrix_dense, linkage_method, metric, 
                            orientation='top', color_threshold=None):
    """ç»˜åˆ¶æ ‘çŠ¶å›¾ï¼ˆæ¢å¤åˆ°è¾ƒç®€å•ç‰ˆæœ¬ï¼‰ã€‚"""
    ax.clear()
    ax.set_facecolor('none') # Set transparent background for axes

    if tfidf_matrix_dense is None or tfidf_matrix_dense.shape[0] < 2:
        ax.text(0.5, 0.5, "æ•°æ®ä¸è¶³ä»¥ç”Ÿæˆæ ‘çŠ¶å›¾", ha="center", va="center", transform=ax.transAxes)
        return

    try:
        linkage_matrix_val = linkage(tfidf_matrix_dense, method=linkage_method, metric=metric)
        
        # Simplified dendrogram plotting, closer to original implicit behavior
        # Default truncation and leaf display will be handled by scipy if not specified, or use simple defaults.
        p_dendro = min(30, tfidf_matrix_dense.shape[0])
        doc_labels = None # No detailed labels by default in this simplified version
        if tfidf_matrix_dense.shape[0] <= p_dendro + 10: # Basic heuristic for showing some labels if few docs
             doc_labels = [f"æ–‡{i}" for i in range(tfidf_matrix_dense.shape[0])]
        
        dendrogram(
            linkage_matrix_val, 
            ax=ax, 
            orientation=orientation, 
            truncate_mode='lastp', # A common default for readability 
            p=p_dendro,              # A common default for readability
            leaf_rotation=90. if orientation in ['top', 'bottom'] else 0., 
            leaf_font_size=8., 
            show_contracted=True,
            labels=doc_labels, # Simplified labels
            color_threshold=color_threshold # Retain color threshold if provided
        )
        
        ax.set_title(f"å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾ ({linkage_method} linkage)", fontsize=14)
        if orientation in ['top', 'bottom']:
            ax.set_xlabel("æ–‡æ¡£ç´¢å¼•æˆ–èšç±»", fontsize=10)
            ax.set_ylabel("è·ç¦»/å·®å¼‚åº¦", fontsize=10)
        else:
            ax.set_ylabel("æ–‡æ¡£ç´¢å¼•æˆ–èšç±»", fontsize=10)
            ax.set_xlabel("è·ç¦»/å·®å¼‚åº¦", fontsize=10)
        
        if color_threshold and color_threshold > 0:
             if orientation in ['top', 'bottom']:
                ax.axhline(y=color_threshold, c='grey', lw=1, linestyle='dashed')
             else: # left, right
                ax.axvline(x=color_threshold, c='grey', lw=1, linestyle='dashed')
        plt.tight_layout()
    except Exception as e_dendro:
        st.error(f"ç”Ÿæˆæ ‘çŠ¶å›¾æ—¶å‡ºé”™: {e_dendro}")
        ax.text(0.5, 0.5, f"ç”Ÿæˆæ ‘çŠ¶å›¾é”™è¯¯: {e_dendro}", ha="center", va="center", transform=ax.transAxes, color='red')

# --- Homepage Rendering Function ---
def render_homepage():
    st.markdown("""
    <div style="text-align: center; padding-top: 1rem; padding-bottom: 1rem;">
        <span style="font-size: 4.5em; line-height: 1;">ğŸš€</span>
        <h1 style="font-size: 3.2em; font-weight: 700; color: #F9FAFB; margin-top: 0.5rem; margin-bottom: 0.75rem; letter-spacing: -0.5px;">
            ä¸­æ–‡NLPæ™ºèƒ½åˆ†æå¹³å°
        </h1>
        <p style="font-size: 1.3em; color: #D1D5DB; max-width: 750px; margin: 0 auto 1.5rem auto; line-height: 1.7;">
            ä¸€ç«™å¼æ»¡è¶³æ‚¨çš„ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†éœ€æ±‚ã€‚æ¢ç´¢æ–‡æœ¬çš„æ·±å±‚ä»·å€¼ï¼Œä»æ™ºèƒ½åˆ†è¯åˆ°é«˜çº§èšç±»åˆ†æï¼Œä½“éªŒå‰æ²¿AIæŠ€æœ¯ã€‚
        </p>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("<hr style='border-top: 1px solid #374151; margin-top: 1rem; margin-bottom: 2rem;'>", unsafe_allow_html=True)
    
    st.markdown("<h2 style='text-align: center; font-size: 2.2em; font-weight: 600; color: #F3F4F6; margin-top: 2rem; margin-bottom: 2.5rem;'>æ ¸å¿ƒåŠŸèƒ½ä¸€è§ˆ âœ¨</h2>", unsafe_allow_html=True)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        <div class="feature-card">
            <h3>ğŸ“ æ™ºèƒ½åˆ†è¯ & è¯é¢‘</h3>
            <p>ç²¾å‡†åˆ‡åˆ†ä¸­æ–‡æ–‡æœ¬ï¼Œç»Ÿè®¡é«˜é¢‘è¯æ±‡ï¼Œå¹¶ç”Ÿæˆç›´è§‚çš„è¯äº‘å›¾å’Œè¯é¢‘ç»Ÿè®¡å›¾ã€‚æ”¯æŒJiebaå’ŒSpaCyå¼•æ“ï¼Œæ·±å…¥æ´å¯Ÿæ–‡æœ¬æ„æˆã€‚</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="feature-card">
            <h3>ğŸ“Š ä¸­æ–‡æ–‡æœ¬åˆ†ç±»</h3>
            <p>å°†æ–‡æœ¬è‡ªåŠ¨å½’ç±»åˆ°é¢„å®šä¹‰ä¸»é¢˜ï¼ˆå¦‚ä½“è‚²ã€è´¢ç»ã€å¨±ä¹ã€ç§‘æŠ€ï¼‰ã€‚æ”¯æŒå¤šç§ç»å…¸åˆ†ç±»ç®—æ³•ï¼Œæä¾›å„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒï¼Œè¾…åŠ©å†³ç­–ã€‚</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="feature-card">
            <h3>ğŸ‘ï¸â€ğŸ—¨ï¸ å‘½åå®ä½“è¯†åˆ« (NER)</h3>
            <p>ä»æ–‡æœ¬ä¸­è‡ªåŠ¨è¯†åˆ«å¹¶åˆ†ç±»å…³é”®å®ä½“ï¼ˆäººåã€åœ°åã€æœºæ„åç­‰ï¼‰ã€‚å¯çµæ´»é€‰æ‹©ä¸åŒè§„æ¨¡çš„SpaCyé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ»¡è¶³ä¸åŒç²¾åº¦éœ€æ±‚ã€‚</p>
        </div>
        """, unsafe_allow_html=True)

        st.markdown("""
        <div class="feature-card">
            <h3>ğŸ§© æ–‡æœ¬èšç±»åˆ†æ</h3>
            <p>æ— ç›‘ç£åœ°å°†ç›¸ä¼¼æ–‡æœ¬è‡ªåŠ¨åˆ†ç»„ï¼Œæ­ç¤ºæ•°æ®é›†ä¸­çš„æ½œåœ¨ç»“æ„ä¸è¯é¢˜ã€‚æä¾›å¤šç§èšç±»ç®—æ³•å’Œä¸°å¯Œçš„å¯è§†åŒ–å·¥å…·ï¼ŒåŠ©åŠ›æ¢ç´¢æ€§æ•°æ®åˆ†æã€‚</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("<hr style='border-top: 1px solid #374151; margin-top: 3rem; margin-bottom: 1.5rem;'><p style='text-align: center; font-size: 1.1em; color: #9CA3AF;'>è¯·ä»å·¦ä¾§å¯¼èˆªæ é€‰æ‹©ä¸€é¡¹åŠŸèƒ½å¼€å§‹æ‚¨çš„åˆ†æä¹‹æ—…ï¼</p>", unsafe_allow_html=True)


# nlp_spacy = load_spacy_model() # This global instance might need to be re-evaluated or removed if model is chosen dynamically per task
# For NER, we will call load_spacy_model with the selected model name.

# --- UI Sections ---
# The following st.title and st.markdown are removed to avoid redundancy with render_homepage() on the homepage.
# For other pages, specific headers like "1. è¾“å…¥æ–‡æœ¬" are used.
# st.title("ä¸­æ–‡NLPæ™ºèƒ½åˆ†æå¹³å° ğŸ¤–") 
# st.markdown("""æ¬¢è¿ä½¿ç”¨æœ¬å¹³å°ï¼è¯·åœ¨ä¸‹æ–¹é€‰æ‹©æ‚¨è¦æ‰§è¡Œçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚
# æ‚¨å¯ä»¥ç›´æ¥ç²˜è´´æ–‡æœ¬æˆ–ä¸Šä¼ TXTæ–‡ä»¶è¿›è¡Œåˆ†æã€‚
# """)

# --- Sidebar for Navigation ---
st.sidebar.header("ğŸ§­ å¯¼èˆª")
analysis_options = [
    "ğŸ  ä¸»é¡µ",
    "ğŸ“ ä¸­æ–‡åˆ†è¯", 
    "ğŸ‘ï¸â€ğŸ—¨ï¸ å‘½åå®ä½“è¯†åˆ«", 
    "ğŸ“Š ä¸­æ–‡æ–‡æœ¬åˆ†ç±»", 
    "ğŸ§© æ–‡æœ¬èšç±»åˆ†æ"
]
selected_analysis = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½:", analysis_options)

# --- Input Area ---
# Moved the condition for showing input area outside homepage
if selected_analysis != "ğŸ  ä¸»é¡µ":
    st.header("âŒ¨ï¸ 1. è¾“å…¥æ–‡æœ¬")
    input_method = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼:", ("ç²˜è´´æ–‡æœ¬", "ä¸Šä¼ TXTæ–‡ä»¶"), horizontal=True, key="input_method_radio")

    raw_texts = []  # ç”¨äºå­˜å‚¨æ‰€æœ‰å¾…å¤„ç†çš„æ–‡æœ¬è¡Œ

    if input_method == "ç²˜è´´æ–‡æœ¬":
        text_area_input = st.text_area("åœ¨æ­¤å¤„ç²˜è´´æ–‡æœ¬ï¼ˆå¯¹äºèšç±»ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªç‹¬ç«‹æ–‡æ¡£ï¼‰:", "", height=200, key="paste_area")
        if text_area_input:
            raw_texts = [line.strip() for line in text_area_input.split('\n') if line.strip()]
    elif input_method == "ä¸Šä¼ TXTæ–‡ä»¶":
        uploaded_files = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªTXTæ–‡ä»¶ (UTF-8ç¼–ç ):", type=["txt"], accept_multiple_files=True, key="file_uploader")
        if uploaded_files:
            for uploaded_file in uploaded_files:
                try:
                    string_data = uploaded_file.read().decode("utf-8")
                    raw_texts.extend([line.strip() for line in string_data.split('\n') if line.strip()])
                except Exception as e:
                    st.error(f"è¯»å–æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {e}")

    # --- Example Data Button ---
    if not raw_texts:
        if st.button("è½½å…¥ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•", key="load_example_button"):
            if selected_analysis == "ğŸ§© æ–‡æœ¬èšç±»åˆ†æ":
                raw_texts = [
                    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒåœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚",
                    "è‡ªç„¶è¯­è¨€å¤„ç†å…³æ³¨è®¡ç®—æœºå¦‚ä½•ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œåº”ç”¨å¹¿æ³›ã€‚",
                    "æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¦‚æ”¯æŒå‘é‡æœºå’Œå†³ç­–æ ‘ï¼Œå¸¸ç”¨äºæ•°æ®æŒ–æ˜ä»»åŠ¡ã€‚",
                    "è‹¹æœå…¬å¸æœ€è¿‘å‘å¸ƒäº†æ–°æ¬¾iPhoneï¼Œé…å¤‡äº†æ›´å¼ºå¤§çš„Aç³»åˆ—ä»¿ç”ŸèŠ¯ç‰‡ã€‚",
                    "ç‰¹æ–¯æ‹‰æ˜¯å…¨çƒé¢†å…ˆçš„ç”µåŠ¨æ±½è½¦åˆ¶é€ å•†ï¼Œå…¶è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å¤‡å—å…³æ³¨ã€‚",
                    "æœ€è¿‘çš„é‡‘èå¸‚åœºæ³¢åŠ¨è¾ƒå¤§ï¼ŒæŠ•èµ„è€…åœ¨è¿›è¡Œè‚¡ç¥¨äº¤æ˜“æ—¶åº”ä¿æŒè°¨æ…ã€‚",
                    "ä¸­å›½å›½å®¶è¶³çƒé˜Ÿæ­£åœ¨ç§¯æå¤‡æˆ˜å³å°†åˆ°æ¥çš„äºšæ´²æ¯é¢„é€‰èµ›ã€‚",
                    "NBAç¯®çƒè”èµ›å¸¸è§„èµ›æ¿€æˆ˜æ­£é…£ï¼Œå„æ”¯çƒé˜Ÿä¸ºå­£åèµ›åé¢å±•å¼€æ¿€çƒˆäº‰å¤ºã€‚"
                ]
                st.toast("å·²åŠ è½½èšç±»ç¤ºä¾‹æ•°æ®ã€‚", icon="ğŸ“„")
            elif selected_analysis == "ğŸ“Š ä¸­æ–‡æ–‡æœ¬åˆ†ç±»":
                raw_texts = ["æœ€æ–°çš„ä½“è‚²æ–°é—»æŠ¥é“äº†æ˜¨æ™šçš„è¶³çƒæ¯”èµ›ç»“æœã€‚"]
                st.toast("å·²åŠ è½½åˆ†ç±»ç¤ºä¾‹æ•°æ®ã€‚", icon="ğŸ“„")
            else: # For word segmentation and NER
                raw_texts = ["æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨ï¼Œå¤©å®‰é—¨ä¸Šå¤ªé˜³å‡ã€‚ä¼Ÿå¤§é¢†è¢–æ¯›ä¸»å¸­ï¼ŒæŒ‡å¼•æˆ‘ä»¬å‘å‰è¿›ã€‚"]
                st.toast("å·²åŠ è½½é€šç”¨ç¤ºä¾‹æ•°æ®ã€‚", icon="ğŸ“„")
else: # if selected_analysis == "ğŸ  ä¸»é¡µ":
    raw_texts = [] # Ensure raw_texts is empty or not used for homepage


# --- Process and Display --- 
current_text_to_process = "" # ç”¨äºå•æ–‡æœ¬åˆ†æä»»åŠ¡
if selected_analysis != "ğŸ  ä¸»é¡µ" and raw_texts: # Ensure this logic only runs for analysis pages with text
    current_text_to_process = raw_texts[0]
    if len(raw_texts) > 1 and selected_analysis not in ["ğŸ§© æ–‡æœ¬èšç±»åˆ†æ"]:
        st.info(f"æ£€æµ‹åˆ° {len(raw_texts)} æ®µæ–‡æœ¬ã€‚å¯¹äº'{selected_analysis}'ï¼Œé»˜è®¤å¤„ç†ç¬¬ä¸€æ®µã€‚å¦‚éœ€æ‰¹é‡å¤„ç†ï¼Œè¯·é€‰æ‹©'æ–‡æœ¬èšç±»åˆ†æ'æˆ–åˆ†åˆ«æ“ä½œã€‚", icon="â„¹ï¸")

if selected_analysis != "ğŸ  ä¸»é¡µ": # Only add divider if not on homepage
    st.divider()

# --- Main logic for different analyses ---
if selected_analysis == "ğŸ  ä¸»é¡µ":
    render_homepage()
    # Footer for homepage can be part of render_homepage or here
    # st.sidebar.markdown("--- "*10) # This is already at the end of the script, keep it there
    # st.sidebar.info("Â© 2024 ä¸­æ–‡NLPæ™ºèƒ½åˆ†æå¹³å°")

elif raw_texts: # For other analysis options, only proceed if raw_texts exist
    st.header("ğŸ” 2. æŸ¥çœ‹åˆ†æç»“æœ")

    # 1. Chinese Word Segmentation
    if selected_analysis == "ğŸ“ ä¸­æ–‡åˆ†è¯":
        if current_text_to_process:
            st.subheader("âœ‚ï¸ ä¸­æ–‡åˆ†è¯ç»“æœ")
            
            # --- Model Selection for Word Segmentation ---
            segmentation_model_options = ["Jieba", "SpaCy"]
            selected_segmentation_model = st.selectbox("é€‰æ‹©åˆ†è¯å¼•æ“:", segmentation_model_options, key="segmentation_model_selector")
            st.write("---åŸå§‹æ–‡æœ¬---") # Changed from st.write("**åŸå§‹æ–‡æœ¬:**", current_text_to_process)
            st.text(current_text_to_process) # Using st.text for better block display of original text
            st.write("---åˆ†è¯ç»“æœ---")

            with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {selected_segmentation_model} è¿›è¡Œåˆ†è¯..."):
                processed_tokens_for_cloud = []
                if selected_segmentation_model == "Jieba":
                    seg_list_jieba = list(jieba.cut(current_text_to_process))
                    st.info(" / ".join(seg_list_jieba))
                    processed_tokens_for_cloud = seg_list_jieba
                    if seg_list_jieba:
                        st.markdown("---")
                        st.markdown("##### ğŸ“Š Top 15 è¯é¢‘ç»Ÿè®¡ (Jieba)")
                        word_counts = pd.Series(seg_list_jieba).value_counts().nlargest(15)
                        if not word_counts.empty:
                            word_counts_df = word_counts.reset_index()
                            word_counts_df.columns = ['è¯è¯­', 'é¢‘ç‡']
                            fig_bar_jieba = px.bar(word_counts_df, 
                                                 x='é¢‘ç‡', 
                                                 y='è¯è¯­', 
                                                 orientation='h', 
                                                 color='è¯è¯­', 
                                                 title="è¯é¢‘ç»Ÿè®¡ (Jieba)",
                                                 labels={'é¢‘ç‡':'é¢‘ç‡', 'è¯è¯­':'è¯è¯­'},
                                                 template="plotly_dark") # Apply dark theme
                            fig_bar_jieba.update_layout(yaxis={'categoryorder':'total ascending'}, showlegend=False)
                            st.plotly_chart(fig_bar_jieba, use_container_width=True)
                        else:
                            st.text("æ— æœ‰æ•ˆè¯è¯­è¿›è¡Œç»Ÿè®¡ã€‚")

                elif selected_segmentation_model == "SpaCy":
                    nlp_segmentation_model = load_spacy_model("zh_core_web_sm") 
                    if nlp_segmentation_model:
                        doc_spacy = nlp_segmentation_model(current_text_to_process)
                        # Filter out punctuation and spaces for cleaner word cloud and frequency count
                        seg_list_spacy = [token.text for token in doc_spacy if not token.is_punct and not token.is_space and token.text.strip()]
                        st.info(" / ".join(seg_list_spacy))
                        processed_tokens_for_cloud = seg_list_spacy
                        if seg_list_spacy:
                            st.markdown("---")
                            st.markdown("##### ğŸ“Š Top 15 è¯é¢‘ç»Ÿè®¡ (SpaCy)")
                            word_counts_spacy = pd.Series(seg_list_spacy).value_counts().nlargest(15)
                            if not word_counts_spacy.empty:
                                word_counts_spacy_df = word_counts_spacy.reset_index()
                                word_counts_spacy_df.columns = ['è¯è¯­', 'é¢‘ç‡']
                                fig_bar_spacy = px.bar(word_counts_spacy_df, 
                                                     x='é¢‘ç‡', 
                                                     y='è¯è¯­', 
                                                     orientation='h', 
                                                     color='è¯è¯­',
                                                     title="è¯é¢‘ç»Ÿè®¡ (SpaCy)",
                                                     labels={'é¢‘ç‡':'é¢‘ç‡', 'è¯è¯­':'è¯è¯­'},
                                                     template="plotly_dark") # Apply dark theme
                                fig_bar_spacy.update_layout(yaxis={'categoryorder':'total ascending'}, showlegend=False)
                                st.plotly_chart(fig_bar_spacy, use_container_width=True)
                            else:
                                st.text("æ— æœ‰æ•ˆè¯è¯­è¿›è¡Œç»Ÿè®¡ã€‚")
                    else:
                        st.error("SpaCyæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ†è¯ã€‚")
                
                # --- Word Cloud Visualization (Common for both Jieba and SpaCy) ---
                if processed_tokens_for_cloud:
                    st.markdown("---")
                    st.markdown("##### â˜ï¸ è¯äº‘å›¾")
                    try:
                        # Try to get font path for WordCloud - simplified and more robust
                        font_path_wc = os.path.join(os.path.dirname(__file__), CHINESE_FONT_FILENAME)
                        
                        if not os.path.exists(font_path_wc):
                            st.warning(f"è¯äº‘æ— æ³•ç”Ÿæˆï¼šæœªåœ¨ 'nlpp/' ç›®å½•ä¸‹æ‰¾åˆ°ä¸­æ–‡å­—ä½“æ–‡ä»¶ '{CHINESE_FONT_FILENAME}'ã€‚è¯·ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨ã€‚")
                            font_path_wc = None # Explicitly set to None if not found

                        if font_path_wc:
                            wordcloud_text = " ".join(processed_tokens_for_cloud)
                            if wordcloud_text.strip():
                                wc = WordCloud(
                                    font_path=font_path_wc, # Directly use the validated path
                                    width=800, 
                                    height=400, 
                                    background_color='white', # Wordcloud background itself
                                    colormap='viridis', # Example colormap, can be changed
                                    collocations=False 
                                ).generate(wordcloud_text)
                                
                                fig_wc, ax_wc = plt.subplots(figsize=(10,5))
                                ax_wc.imshow(wc, interpolation='bilinear')
                                ax_wc.axis("off")
                                # For dark theme Streamlit, ensure Matplotlib plot bg is transparent or matches
                                fig_wc.patch.set_alpha(0) # Make Matplotlib figure background transparent
                                ax_wc.patch.set_alpha(0)  # Make Matplotlib axes background transparent
                                st.pyplot(fig_wc)
                            else:
                                st.text("æ–‡æœ¬å†…å®¹è¿‡å°‘æˆ–æ— æœ‰æ•ˆè¯è¯­ï¼Œæ— æ³•ç”Ÿæˆè¯äº‘ã€‚")
                        # No else needed here, warning for font_path_wc=None is handled above
                                
                    except Exception as e_wc:
                        st.error(f"ç”Ÿæˆè¯äº‘æ—¶å‡ºé”™: {e_wc}")
        else:
            st.info("è¯·è¾“å…¥æˆ–ä¸Šä¼ æ–‡æœ¬ä»¥è¿›è¡Œåˆ†è¯ã€‚", icon="ğŸ‘ˆ")

    # 2. Named Entity Recognition (NER)
    elif selected_analysis == "ğŸ‘ï¸â€ğŸ—¨ï¸ å‘½åå®ä½“è¯†åˆ«":
        if current_text_to_process:
            st.subheader("ğŸ·ï¸ å‘½åå®ä½“è¯†åˆ« (NER) ç»“æœ")

            # --- Model Selection for NER ---
            ner_model_options = {
                "å°æ¨¡å‹ (zh_core_web_sm)": "zh_core_web_sm",
                "ä¸­ç­‰æ¨¡å‹ (zh_core_web_md)": "zh_core_web_md",
                "å¤§æ¨¡å‹ (zh_core_web_lg)": "zh_core_web_lg",
            }
            selected_ner_model_display_name = st.selectbox(
                "é€‰æ‹©SpaCy NERæ¨¡å‹ (å¤§æ¨¡å‹é¦–æ¬¡åŠ è½½è¾ƒæ…¢ï¼Œéœ€å·²ä¸‹è½½):", 
                list(ner_model_options.keys()), 
                key="ner_model_selector"
            )
            selected_ner_model_name = ner_model_options[selected_ner_model_display_name]

            # Load the selected spaCy model for NER
            # The load_spacy_model function is cached, so it will be efficient after the first load of a model.
            nlp_ner_model = load_spacy_model(selected_ner_model_name)

            if nlp_ner_model is None:
                # Error handling is done in load_spacy_model, which calls st.stop()
                # So, execution should not reach here if a model fails to load.
                st.error("NERæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚") # Fallback, should not be seen normally
            else:
                with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {selected_ner_model_display_name} è¿›è¡ŒNERåˆ†æ..."):
                    st.write("**åŸå§‹æ–‡æœ¬:**", current_text_to_process)
                    doc = nlp_ner_model(current_text_to_process)
                    
                    # Get labels from the current model's NER pipe
                    ner_labels = []
                    if nlp_ner_model.has_pipe("ner"):
                        ner_labels = list(nlp_ner_model.get_pipe("ner").labels)
                    
                    st.subheader("ğŸ”¦ é«˜äº®å®ä½“:") # Added emoji and using subheader for emphasis
                    visualize_ner(doc, labels=ner_labels, show_table=False, title="", displacy_options={"colors": {"ORG": "#7DF9FF", "PERSON": "#FFC0CB", "LOC": "#LIGHTGREEN", "GPE":"#FFD700" }})
                    
                    if doc.ents:
                        st.markdown("#### ğŸ“‹ è¯†åˆ«åˆ°çš„å®ä½“åˆ—è¡¨")
                        entities = [(ent.text, ent.label_) for ent in doc.ents]
                        df_ents = pd.DataFrame(entities, columns=["å®ä½“æ–‡æœ¬", "å®ä½“ç±»å‹"])
                        st.dataframe(df_ents, use_container_width=True)

                        if not df_ents.empty:
                            st.markdown("---")
                            st.markdown("##### ğŸ“Š å®ä½“ç±»å‹ç»Ÿè®¡")
                            entity_type_counts = df_ents["å®ä½“ç±»å‹"].value_counts().reset_index()
                            entity_type_counts.columns = ['å®ä½“ç±»å‹', 'æ•°é‡'] # Rename for Plotly
                            if not entity_type_counts.empty:
                                fig_ner_counts = px.bar(entity_type_counts, 
                                                        x='å®ä½“ç±»å‹', 
                                                        y='æ•°é‡', 
                                                        color='å®ä½“ç±»å‹',
                                                        title="å‘½åå®ä½“ç±»å‹åˆ†å¸ƒ",
                                                        labels={'å®ä½“ç±»å‹':'å®ä½“ç±»å‹', 'æ•°é‡':'å‡ºç°æ¬¡æ•°'},
                                                        template="plotly_dark") # Apply dark theme
                                fig_ner_counts.update_layout(xaxis_title="å®ä½“ç±»å‹", yaxis_title="æ•°é‡", showlegend=True)
                                st.plotly_chart(fig_ner_counts, use_container_width=True)
                            else:
                                st.text("æœªè¯†åˆ«åˆ°å¯ç»Ÿè®¡çš„å®ä½“ç±»å‹ã€‚")
                    else:
                        st.info("æœªåœ¨æœ¬æ®µæ–‡æœ¬ä¸­è¯†åˆ«åˆ°å‘½åå®ä½“ã€‚")
        else:
            st.info("è¯·è¾“å…¥æˆ–ä¸Šä¼ æ–‡æœ¬ä»¥è¿›è¡ŒNERåˆ†æã€‚", icon="ğŸ‘ˆ")

    # 3. Chinese Text Classification
    elif selected_analysis == "ğŸ“Š ä¸­æ–‡æ–‡æœ¬åˆ†ç±»":
        if current_text_to_process:
            st.subheader("ğŸ¯ ä¸­æ–‡æ–‡æœ¬åˆ†ç±»ç»“æœ")

            # --- Classifier Model Selection ---
            classifier_options = {
                "æœ´ç´ è´å¶æ–¯ (MultinomialNB)": "MultinomialNB",
                "é€»è¾‘å›å½’ (LogisticRegression)": "LogisticRegression",
                "æ”¯æŒå‘é‡æœº (LinearSVC)": "LinearSVC"
            }
            selected_classifier_display_name = st.selectbox(
                "é€‰æ‹©åˆ†ç±»ç®—æ³•:",
                list(classifier_options.keys()),
                key="classifier_selector"
            )
            selected_classifier_type = classifier_options[selected_classifier_display_name]

            # Train or load the selected classifier from cache
            classifier_model = train_text_classifier(classification_texts, classification_labels, classifier_choice=selected_classifier_type)

            if classifier_model:
                with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {selected_classifier_display_name} è¿›è¡Œæ–‡æœ¬åˆ†ç±»..."):
                    st.write("**åˆ†ææ–‡æœ¬:**", current_text_to_process[:200] + ("..." if len(current_text_to_process) > 200 else ""))
                    prediction = classifier_model.predict([current_text_to_process])[0]
                    
                    st.success(f"**é¢„æµ‹ç±»åˆ«:** {prediction}")

                    # Check if the classifier has predict_proba method
                    if hasattr(classifier_model, "predict_proba") and callable(classifier_model.predict_proba):
                        probabilities = classifier_model.predict_proba([current_text_to_process])[0]
                        prob_df = pd.DataFrame({'ç±»åˆ«': class_names, 'æ¦‚ç‡': probabilities})
                        prob_df = prob_df.sort_values(by='æ¦‚ç‡', ascending=False).reset_index(drop=True)
                        st.markdown("#### ğŸ“ˆ å„ç±»åˆ«æ¦‚ç‡")
                        st.dataframe(prob_df, use_container_width=True)

                        st.markdown("---")
                        # Gauge chart for the top prediction
                        if not prob_df.empty:
                            top_prediction_label = prob_df.iloc[0]['ç±»åˆ«']
                            top_prediction_prob = prob_df.iloc[0]['æ¦‚ç‡']
                            
                            st.markdown(f"##### ğŸ¯ ä¸»è¦é¢„æµ‹ç±»åˆ« ({top_prediction_label}) ç½®ä¿¡åº¦")
                            fig_gauge = go.Figure(go.Indicator(
                                mode = "gauge+number",
                                value = top_prediction_prob * 100, # Convert to percentage
                                title = {'text': f"é¢„æµ‹ä¸º: {top_prediction_label}", 'font': {'color': "white"}},
                                gauge = {'axis': {'range': [None, 100]},
                                         'bar': {'color': "#636EFA"}, # Using a color from plotly_dark
                                         'steps' : [
                                             {'range': [0, 50], 'color': "rgba(255,255,255,0.1)"},
                                             {'range': [50, 80], 'color': "rgba(255,255,255,0.2)"}],
                                         'threshold' : {'line': {'color': "red", 'width': 4}, 'thickness': 0.75, 'value': 90}},
                                number = {'font': {'color': "white"}}
                                ))
                            fig_gauge.update_layout(height=300, margin=dict(t=50, b=50, l=50, r=50), template="plotly_dark") # Apply dark theme
                            st.plotly_chart(fig_gauge, use_container_width=True)

                        # Visualizing probabilities with Plotly bar chart
                        st.markdown("---")
                        st.markdown("##### ğŸ“Š å„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒå›¾")
                        if not prob_df.empty:
                            fig_class_probs = px.bar(prob_df, 
                                                     x='æ¦‚ç‡', 
                                                     y='ç±»åˆ«', 
                                                     orientation='h',
                                                     color='ç±»åˆ«',
                                                     title="æ–‡æœ¬åˆ†ç±»å„ç±»åˆ«æ¦‚ç‡",
                                                     labels={'æ¦‚ç‡':'é¢„æµ‹æ¦‚ç‡', 'ç±»åˆ«':'æ–‡æœ¬ç±»åˆ«'},
                                                     template="plotly_dark") # Apply dark theme
                            fig_class_probs.update_layout(yaxis={'categoryorder':'total ascending'}, 
                                                      showlegend=False,
                                                      xaxis_ticksuffix="%") # Add % suffix to probability axis
                            # Convert probability to percentage for display on hover and ticks if desired
                            # fig_class_probs.update_traces(x=[val * 100 for val in prob_df['æ¦‚ç‡']])
                            st.plotly_chart(fig_class_probs, use_container_width=True)
                        else:
                            st.text("æ— æ¦‚ç‡ä¿¡æ¯å¯ä¾›å¯è§†åŒ–ã€‚")
                    elif selected_classifier_type == "LinearSVC":
                        st.info(f"æ³¨æ„: {selected_classifier_display_name} æ¨¡å‹ä¸ç›´æ¥æä¾›æ¦‚ç‡è¾“å‡ºã€‚å†³ç­–å‡½æ•°å€¼å¯ç”¨äºè¯„ä¼°ç½®ä¿¡åº¦ï¼Œä½†æ­¤å¤„æœªæ˜¾ç¤ºã€‚")
                    else:
                        st.warning(f"å½“å‰é€‰æ‹©çš„åˆ†ç±»å™¨ {selected_classifier_display_name} ä¸æ”¯æŒæ¦‚ç‡è¾“å‡ºã€‚")
            else:
                st.error("åˆ†ç±»å™¨æ¨¡å‹æœªèƒ½æˆåŠŸåŠ è½½æˆ–è®­ç»ƒã€‚")
                
        else:
            st.info("è¯·è¾“å…¥æˆ–ä¸Šä¼ æ–‡æœ¬ä»¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚åªæœ‰åœ¨è¾“å…¥å•ä¸ªæ–‡æœ¬æ®µè½æ—¶ï¼Œæ­¤åŠŸèƒ½æ‰å¯ç”¨ã€‚", icon="ğŸ‘ˆ")

    # 4. Text Clustering Analysis
    elif selected_analysis == "ğŸ§© æ–‡æœ¬èšç±»åˆ†æ":
        if len(raw_texts) < 2:
            st.warning("æ–‡æœ¬èšç±»è‡³å°‘éœ€è¦ä¸¤æ¡æ–‡æœ¬ã€‚è¯·ç²˜è´´å¤šè¡Œæ–‡æœ¬æˆ–ä¸Šä¼ åŒ…å«å¤šè¡Œ/å¤šä¸ªæ–‡ä»¶ã€‚", icon="âš ï¸")
        else:
            st.subheader("âš™ï¸ æ–‡æœ¬èšç±»åˆ†æå‚æ•°è®¾ç½®")
            cluster_model_name = st.selectbox(
                "é€‰æ‹©èšç±»æ¨¡å‹:",
                ("KMeans", "AgglomerativeClustering", "DBSCAN", "BIRCH"),
                key="cluster_model_select"
            )
            # Initialize clusters to None before the try block
            clusters = None 
            model_instance = None
            num_clusters_found = 0
            tfidf_matrix_dense = None # Ensure it's defined for later use, even if try block fails early
            linkage_agg = 'ward' # Default for Agglomerative, if needed before set
            metric_agg = 'euclidean' # Default for Agglomerative

            try:
                tfidf_vectorizer = TfidfVectorizer(
                    tokenizer=lambda x: list(jieba.cut(x)), max_df=0.90, min_df=2, stop_words=None
                )
                tfidf_matrix = tfidf_vectorizer.fit_transform(raw_texts)
                tfidf_matrix_dense = tfidf_matrix.toarray() 

                if tfidf_matrix.shape[0] < 2 or tfidf_matrix.shape[1] < 1:
                    st.error("ç»è¿‡TF-IDFå¤„ç†åï¼Œæ–‡æœ¬æ•°æ®ä¸è¶³æˆ–ç‰¹å¾è¿‡å°‘ã€‚")
                    st.stop()
                
                max_possible_clusters = min(tfidf_matrix.shape[0] - 1 if tfidf_matrix.shape[0] > 1 else 1, 20)
                if max_possible_clusters < 1 and cluster_model_name not in ["DBSCAN"]:
                     st.warning("æ–‡æœ¬æ•°é‡è¿‡å°‘ï¼Œæ— æ³•æœ‰æ•ˆèšç±»ã€‚")
                     st.stop()
                
                # --- Model Specific Parameters and Execution (copied from your last correct version) ---
                if cluster_model_name == "KMeans":
                    st.markdown("##### âœ¨ KMeans å‚æ•°")
                    num_clusters_kmeans = st.slider("Kå€¼:", min_value=2, max_value=max_possible_clusters if max_possible_clusters >=2 else 2, value=min(4, max_possible_clusters) if max_possible_clusters >=2 else 2, key="k_kmeans")
                    if tfidf_matrix.shape[0] < num_clusters_kmeans: st.error("æ–‡æœ¬æ•°å°‘äºKå€¼!"); st.stop()
                    with st.spinner(f"KMeans (K={num_clusters_kmeans}) èšç±»ä¸­..."):
                        model_instance = KMeans(n_clusters=num_clusters_kmeans, random_state=42, n_init='auto')
                        clusters = model_instance.fit_predict(tfidf_matrix)
                        num_clusters_found = num_clusters_kmeans
                elif cluster_model_name == "AgglomerativeClustering":
                    st.markdown("##### ğŸ”— Agglomerative Clustering å‚æ•°")
                    num_clusters_agg = st.slider("ç°‡æ•°é‡:", min_value=2, max_value=max_possible_clusters if max_possible_clusters >=2 else 2, value=min(4, max_possible_clusters) if max_possible_clusters >=2 else 2, key="k_agg")
                    linkage_agg = st.selectbox("Linkageæ–¹æ³•:", ('ward', 'complete', 'average', 'single'), key="linkage_agg")
                    metric_agg = 'euclidean' 
                    if tfidf_matrix.shape[0] < num_clusters_agg: st.error("æ–‡æœ¬æ•°å°‘äºç°‡æ•°é‡!"); st.stop()
                    with st.spinner(f"Agglomerative (k={num_clusters_agg}, linkage={linkage_agg}) èšç±»ä¸­..."):
                        model_instance = AgglomerativeClustering(n_clusters=num_clusters_agg, metric=metric_agg, linkage=linkage_agg)
                        clusters = model_instance.fit_predict(tfidf_matrix_dense)
                        num_clusters_found = num_clusters_agg
                elif cluster_model_name == "DBSCAN":
                    st.markdown("##### ğŸ§ DBSCAN å‚æ•°")
                    eps_dbscan = st.number_input("Epsilon (eps - é‚»åŸŸåŠå¾„):", min_value=0.01, value=0.5, step=0.01, format="%.2f", key="eps_dbscan")
                    min_samples_dbscan = st.slider("Min Samples (æ ¸å¿ƒå¯¹è±¡æœ€å°æ ·æœ¬æ•°):", min_value=1, max_value=max(5, tfidf_matrix.shape[0] // 10), value=max(1,min(5, tfidf_matrix.shape[0] // 10 if tfidf_matrix.shape[0] // 10 >0 else 1)), key="min_samples_dbscan")
                    with st.spinner(f"DBSCAN (eps={eps_dbscan}, min_samples={min_samples_dbscan}) èšç±»ä¸­..."):
                        model_instance = DBSCAN(eps=eps_dbscan, min_samples=min_samples_dbscan, metric='cosine')
                        clusters = model_instance.fit_predict(tfidf_matrix)
                        num_clusters_found = len(set(clusters)) - (1 if -1 in clusters else 0)
                        st.info(f"DBSCAN æ‰¾åˆ° {num_clusters_found} ä¸ªç°‡å’Œ {list(clusters).count(-1)} ä¸ªç¦»ç¾¤ç‚¹ã€‚")
                elif cluster_model_name == "BIRCH":
                    st.markdown("##### ğŸŒ³ BIRCH å‚æ•°")
                    threshold_birch = st.number_input("Threshold (CFå­ç°‡åŠå¾„é˜ˆå€¼):", min_value=0.01, value=0.5, step=0.05, format="%.2f", key="threshold_birch")
                    branching_factor_birch = st.slider("Branching Factor (CFæ ‘åˆ†æ”¯å› å­):", min_value=2, max_value=100, value=50, key="branching_birch")
                    n_clusters_birch_options = [None] + list(range(2, (max_possible_clusters if max_possible_clusters >=2 else 2) + 1))
                    default_k_birch = min(3, max_possible_clusters) if max_possible_clusters >=2 else None
                    if default_k_birch == None and len(n_clusters_birch_options) > 1: default_k_birch = 2
                    num_clusters_birch_selected = st.select_slider("ç›®æ ‡ç°‡æ•°é‡ (K - Noneåˆ™è‡ªåŠ¨ç¡®å®š):", options=n_clusters_birch_options, value=default_k_birch, key="k_birch_slider")
                    with st.spinner(f"BIRCH (threshold={threshold_birch}, n_clusters={num_clusters_birch_selected}) èšç±»ä¸­..."):
                        model_instance = Birch(threshold=threshold_birch, branching_factor=branching_factor_birch, n_clusters=num_clusters_birch_selected)
                        clusters = model_instance.fit_predict(tfidf_matrix)
                        if hasattr(model_instance, 'n_clusters_') and model_instance.n_clusters_ is not None: num_clusters_found = model_instance.n_clusters_
                        elif num_clusters_birch_selected is not None: num_clusters_found = num_clusters_birch_selected
                        else: num_clusters_found = len(set(clusters))
                        st.info(f"BIRCH æ‰¾åˆ°/è®¾å®š {num_clusters_found} ä¸ªç°‡ã€‚")

            except ValueError as ve:
                st.error(f"èšç±»åˆ†ææ—¶å‘ç”Ÿæ•°å€¼é”™è¯¯: {ve}")
                clusters = None # Ensure clusters is None if an error occurs
            except Exception as e:
                st.error(f"èšç±»åˆ†ææ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                clusters = None # Ensure clusters is None if an error occurs

            # --- Displaying Results & Metrics & Visualizations (common for all models) ---
            # This block will only execute if clusters were successfully computed (clusters is not None)
            if clusters is not None:
                st.subheader("ğŸ“Š æ–‡æœ¬èšç±»åˆ†æç»“æœ")
                df_cluster_results = pd.DataFrame({'åŸå§‹æ–‡æœ¬': raw_texts, 'èšç±»æ ‡ç­¾': clusters})
                st.markdown("#### ğŸ“‹ æ–‡æœ¬èšç±»åˆ†é…:")
                st.dataframe(df_cluster_results, height=200, use_container_width=True)

                st.markdown("#### ğŸ“ èšç±»è¯„ä¼°æŒ‡æ ‡:")
                if num_clusters_found >= 2 and num_clusters_found < tfidf_matrix.shape[0]:
                    try:
                        sil_score = silhouette_score(tfidf_matrix, clusters, metric='cosine')
                        db_score = davies_bouldin_score(tfidf_matrix_dense, clusters)
                        st.metric(label="è½®å»“ç³»æ•° (Silhouette Score)", value=f"{sil_score:.3f}", help="èŒƒå›´[-1, 1]ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½ã€‚ä½¿ç”¨ä½™å¼¦è·ç¦»è®¡ç®—ã€‚")
                        st.metric(label="æˆ´ç»´æ–¯-å¸ƒå°”ä¸æŒ‡æ•° (Davies-Bouldin)", value=f"{db_score:.3f}", help="å€¼è¶Šå°è¶Šå¥½ã€‚")
                    except ValueError as e_metric: st.warning(f"è®¡ç®—è¯„ä¼°æŒ‡æ ‡æ—¶å‡ºé”™: {e_metric}ã€‚");
                    except Exception as e_metric_other: st.error(f"è®¡ç®—è¯„ä¼°æŒ‡æ ‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e_metric_other}")
                else:
                    st.info(f"æ‰¾åˆ° {num_clusters_found} ä¸ªç°‡ã€‚è¯„ä¼°æŒ‡æ ‡åœ¨ç°‡æ•°é‡ä¸º 2 åˆ° (æ ·æœ¬æ•°-1) ä¹‹é—´æ—¶æ›´æœ‰æ„ä¹‰ã€‚")
                
                st.markdown("### ğŸ–¼ï¸ å¯è§†åŒ–å±•æ¿") # Was "--- Visualizations ---"
                with st.expander("é™ç»´æ•£ç‚¹å›¾è®¾ç½®ä¸å¯è§†åŒ–", expanded=True):
                    st.markdown("#### ğŸ“ˆ é™ç»´æ•£ç‚¹å›¾") # Was "**èšç±»ç»“æœå¯è§†åŒ– (é™ç»´æ•£ç‚¹å›¾):**"
                    
                    vis_col_main_1, vis_col_main_2 = st.columns([2,1]) # Main columns for scatter controls

                    with vis_col_main_1: # Left column for primary controls
                        reduction_method = st.selectbox("é™ç»´æ–¹æ³•:", ["PCA", "t-SNE"], key="reduction_scatter")
                        available_colormaps = plt.colormaps()
                        filtered_colormaps = [cm for cm in available_colormaps if not cm.endswith('_r')]
                        selected_colormap = st.selectbox("é€‰æ‹©è°ƒè‰²æ¿:", filtered_colormaps, index=filtered_colormaps.index('viridis') if 'viridis' in filtered_colormaps else 0, key="colormap_scatter")
                        scatter_marker_styles_enabled = st.checkbox("ä¸ºä¸åŒç°‡å¯ç”¨ä¸åŒæ ‡è®°æ ·å¼", False, key="scatter_marker_styles_cb")
                        scatter_show_grid = st.checkbox("æ˜¾ç¤ºç½‘æ ¼", True, key="scatter_show_grid_cb")
                        
                    with vis_col_main_2: # Right column for size, alpha and colors
                        scatter_marker_size = st.slider("æ ‡è®°å¤§å°:", min_value=10, max_value=200, value=50, step=10, key="scatter_marker_size_slider")
                        scatter_marker_alpha = st.slider("æ ‡è®°é€æ˜åº¦:", min_value=0.1, max_value=1.0, value=0.7, step=0.05, key="scatter_marker_alpha_slider")
                        
                        # Horizontal layout for background and grid color pickers
                        color_col1, color_col2 = st.columns(2)
                        with color_col1:
                            scatter_bg_color = st.color_picker("èƒŒæ™¯é¢œè‰²", "#1E293B", key="scatter_bg_color_picker")
                        with color_col2:
                            if scatter_show_grid:
                                scatter_grid_color = st.color_picker("ç½‘æ ¼é¢œè‰²", "#4B5563", key="scatter_grid_color_picker")
                            else:
                                scatter_grid_color = "#4B5563" # Default even if not shown, to prevent error
                    
                    perplexity_value = min(30.0, float(max(1, tfidf_matrix_dense.shape[0] - 2)))
                    if perplexity_value < 5: perplexity_value = 5
                    n_components_for_reduction = 2
                    
                    pca_explained_variance_ratio = None # Initialize

                    if tfidf_matrix_dense.shape[1] < n_components_for_reduction:
                        st.warning(f"TF-IDFç‰¹å¾æ•° ({tfidf_matrix_dense.shape[1]}) å°‘äºé™ç»´ç›®æ ‡ ({n_components_for_reduction})ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆé™ç»´å¯è§†åŒ–ã€‚")
                        reduced_features = tfidf_matrix_dense[:, :tfidf_matrix_dense.shape[1]] 
                        if tfidf_matrix_dense.shape[1] == 1: 
                             reduced_features = np.hstack((reduced_features, np.zeros_like(reduced_features)))
                        elif tfidf_matrix_dense.shape[1] == 0:
                             reduced_features = np.zeros((tfidf_matrix_dense.shape[0], 2)) 
                    else:
                        if reduction_method == "PCA":
                            reducer = PCA(n_components=n_components_for_reduction, random_state=42)
                            reduced_features = reducer.fit_transform(tfidf_matrix_dense)
                            pca_explained_variance_ratio = reducer.explained_variance_ratio_
                        else: # t-SNE
                            reducer = TSNE(n_components=n_components_for_reduction, random_state=42, perplexity=perplexity_value, n_iter=300, init='pca', learning_rate='auto')
                            reduced_features = reducer.fit_transform(tfidf_matrix_dense)
                    
                    fig_scatter, ax_scatter = plt.subplots(figsize=(8, 6)) 
                    fig_scatter.patch.set_alpha(0) # Ensure figure background is transparent
                    # ax_scatter.patch.set_alpha(0) # ax facecolor is handled by plot_scatter_clusters bg_color

                    plot_scatter_clusters(ax_scatter, reduced_features, clusters, cluster_model_name,
                                          colormap=selected_colormap, 
                                          marker_size=scatter_marker_size, 
                                          marker_alpha=scatter_marker_alpha,
                                          marker_styles_enabled=scatter_marker_styles_enabled,
                                          bg_color=scatter_bg_color, 
                                          show_grid=scatter_show_grid, 
                                          grid_color=scatter_grid_color)
                    ax_scatter.set_title(f"{reduction_method} å¯è§†åŒ– ({cluster_model_name})", fontsize=14)
                    st.pyplot(fig_scatter)

                    if pca_explained_variance_ratio is not None:
                        st.markdown("#### ğŸ’¡ PCA è§£é‡Šæ–¹å·®è´¡çŒ®ï¼š")
                        pca_col1, pca_col2, pca_col3 = st.columns(3)
                        pca_col1.metric("ä¸»æˆåˆ† 1", f"{pca_explained_variance_ratio[0]:.2%}")
                        if len(pca_explained_variance_ratio) > 1:
                            pca_col2.metric("ä¸»æˆåˆ† 2", f"{pca_explained_variance_ratio[1]:.2%}")
                            pca_col3.metric("ç´¯è®¡è´¡çŒ®", f"{np.sum(pca_explained_variance_ratio):.2%}")
                        else:
                            pca_col2.metric("ç´¯è®¡è´¡çŒ®", f"{np.sum(pca_explained_variance_ratio):.2%}")

                if cluster_model_name == "AgglomerativeClustering":
                    with st.expander("å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾è®¾ç½®ä¸å¯è§†åŒ–"):
                        st.markdown("#### ğŸŒ³ å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾") # Was "**å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾:**"
                        current_linkage_method = locals().get('linkage_agg', 'ward') 
                        current_metric = locals().get('metric_agg', 'euclidean')
                        
                        # Simplified dendrogram controls
                        dendro_orientation = st.selectbox("æ ‘çŠ¶å›¾æ–¹å‘:", ['top', 'bottom', 'left', 'right'], index=0, key="dendro_orientation_simple_select")
                        default_color_thresh_simple = 0.7 * linkage(tfidf_matrix_dense, method=current_linkage_method, metric=current_metric)[:,2].max() if tfidf_matrix_dense is not None and tfidf_matrix_dense.size > 0 and tfidf_matrix_dense.shape[0] >1 else 0.0
                        dendro_color_threshold_simple = st.number_input("é¢œè‰²é˜ˆå€¼ (è·ç¦», 0ç¦ç”¨):", min_value=0.0, value=default_color_thresh_simple, step=0.1, key="dendro_color_thr_simple_input")
                        if dendro_color_threshold_simple <= 0: dendro_color_threshold_simple = None
                        
                        with st.spinner("ç”Ÿæˆæ ‘çŠ¶å›¾ä¸­..."):
                            fig_dendro, ax_dendro = plt.subplots(figsize=(10, 7)) # Reverted to fixed height
                            plot_dynamic_dendrogram(ax_dendro, tfidf_matrix_dense, 
                                                    linkage_method=current_linkage_method, 
                                                    metric=current_metric,
                                                    orientation=dendro_orientation, 
                                                    color_threshold=dendro_color_threshold_simple)
                            st.pyplot(fig_dendro)
                            
            else: # This 'else' pairs with 'if clusters is not None:'
                # This will be shown if the clustering process in the try-except block failed and clusters remained None,
                # or if it's the initial state before any clustering is attempted for the current raw_texts.
                if selected_analysis == "ğŸ§© æ–‡æœ¬èšç±»åˆ†æ": # Only show this if we are in the clustering section
                    st.info("è¯·é…ç½®å‚æ•°å¹¶è¿è¡Œèšç±»æ¨¡å‹ï¼Œç»“æœå’Œå¯è§†åŒ–å°†åœ¨æ­¤æ˜¾ç¤ºã€‚")

else: # No raw_texts AND not on homepage
    if selected_analysis and selected_analysis != "ğŸ  ä¸»é¡µ": # Only show if an analysis was selected but no text
         st.info("è¯·åœ¨ä¸Šæ–¹ç²˜è´´æ–‡æœ¬ã€ä¸Šä¼ æ–‡ä»¶æˆ–è½½å…¥ç¤ºä¾‹æ•°æ®ä»¥å¼€å§‹åˆ†æã€‚", icon="â¬†ï¸")

st.sidebar.markdown("--- "*10)
st.sidebar.info("Â© 2024 ä¸­æ–‡NLPæ™ºèƒ½åˆ†æå¹³å°") 